#+TITLE: HPI: Fission talk
#+AUTHOR: Dima (@karlicoss)
#+OPTIONS: toc:0
#+REVEAL_EXTRA_CSS: ./style.css

# meta-structure:

# 1. intro (??)
# 2. motivation/story
# 3. prior art and how HPI is different (focus on interop rather than UI)
# 4. architecture?
# 5. what's next?


* HPI
Human Programming Interface, programmatic offline API to your personal data.

** TODO [#A] quick demo :noexport:
# TODO maybe show excerpt from my post on HR correlation?
# (somewhat confusingly, the package name is =my=, so everywhere you see =import my=, it means "HPI")
# the basic idea is that I can just import the data as python objects (although it doesn't have to be python)

TODO fuck, what examplt to add here??

maybe discord?
shorten the messages maybe?
: hpi query my.discord.messages -o pprint | grep HPI | head -n 3                                                                                                                                                                                 22:06:01
:  Message(message_id=800446427100348456, timestamp=datetime.datetime(2021, 1, 17, 19, 28, 19, 796000, tzinfo=datetime.timezone.utc), channel=Channel(channel_id=800189382837534720, name='promnesia', server=Server(server_id=727903265437777944, name='The Productivists')), content='https://github.com/karlicoss/HPI/blob/master/doc/SETUP.org#data-flow this might also clarify some things', attachments=''),
:  Message(message_id=800446094458617856, timestamp=datetime.datetime(2021, 1, 17, 19, 27, 0, 488000, tzinfo=datetime.timezone.utc), channel=Channel(channel_id=800189382837534720, name='promnesia', server=Server(server_id=727903265437777944, name='The Productivists')), content="I've gotta go now, but read up on HPI in the meantime! Or if you're interested in reddit data in particular you can start with setting up https://github.com/karlicoss/rexport#setting-up", attachments=''),
:  Message(message_id=800445310596808705, timestamp=datetime.datetime(2021, 1, 17, 19, 23, 53, 601000, tzinfo=datetime.timezone.utc), channel=Channel(channel_id=800189382837534720, name='promnesia', server=Server(server_id=727903265437777944, name='The Productivists')), content="you might need `pip3 install --user promnesia[HPI]` , but I recommend reading up on HPI first just to understand how it's all set up", attachments=''),
composable with familiar tools
** demo
# interactive, use as code
- https://beepb00p.xyz/hpi.html#interactive
- https://beepb00p.xyz/myinfra-roam.html#interactive
- https://beepb00p.xyz/heartbeats_vs_kcals.html
** demo
- promnesia: https://github.com/karlicoss/promnesia#demos
  # the major consumer perhaps
- visidata: https://twitter.com/karlicoss/status/1373905988563132417
- grafana
  - https://twitter.com/karlicoss/status/1361100437332590593
  - https://twitter.com/karlicoss/status/1360369025122000898
- orger?
  # press f2 to demonstrate
  # maybe materialistic module?
- live demo in ipython later?
** modules
# just a quick overview, not sure if there is much to talk about here
Or 'data sources/providers'.
- https://beepb00p.xyz/hpi.html#modules
- [[file:/data/blog/hpi.org::*What's inside?][modules]]
  # could say 'fun fact -- this list is programmatically generated by HPI'
- a simple module is just a python file
- extensible!
  # click
  https://github.com/seanbreckenridge/HPI#my-modules

* story
Software should enhance and augment human mental capabilities.
Computers are very good at

- long term storage & recall
- search
- processing & analysis
- organizing

# As a programmer of course I chose to solve these problems by writing more code.
# although with time I'm getting more and more convinced that some of these should be regulated

** sad reality
https://beepb00p.xyz/sad-infra.html
- Tons of apps & services, barely interoperating
  - instant messaging
  - social networks
  - content networks (e.g. youtube)
  - e-books
  - phone apps
- The data is siloed and trapped
- User interfaces are primitive
  # Also suck and impossible to tweak.
  # optimize for 'engagement'
- Data insights we get are laughable and blackboxy
  # quantified self?

** STRT ? :noexport:
- Why can't I have incremental search over my tweets?
  Or browser bookmarks? Or over everything I've ever typed/read on the Internet?
- Why can't I easily see what were the books/music/art recommended by my friends or some specific Twitter/Reddit/Hackernews users?
- Where did I write that comment? Reddit, Github or Twitter?
- When did I travel abroad (e.g. for visa purposes)
- How

** hypothesis
Unifying the data behind a local programmatic interface would make it much easier
- exploring
- building new tools
- extending

* why local?
# (offline even!)
REST apis suck.

https://beepb00p.xyz/sad-infra.html#exports_are_hard
- auth
- pagination
- consistency
- rate limits
- error handling

* why programmatic?
** code is inevitable
- parsing & normalising
- data validation
- migrations

** code is awesome!
- composable
- interoperable
- possible to abstract
- extendable
  # possible to enrich data
- flexible error handling
  # even if you don't test the code, worst thing that could happen is crashing
  # acceptable for our purposes
  # data is always messy
*** TODO mention about python configs? :noexport:
https://beepb00p.xyz/configs-suck.html
- just 'import' the configs and you're all set
- configs are flexible
- free linting tools: =mypy= , =pylint=, etc.
- security is not a concern here

** you don't always need 'apps'
Any app inevitably restricts you, imposing a schema.
# in comparison apps often force fixes set of fields on you, restricting the context etc
- plaintext input, e.g. markdown/org-mode/csv, just in your text editor
  # often you figure out the best schema in process, you can't predict it in advance
  # e.g. exercise tracking, depending on the exercise you do it might be different
- track/input data first, parse later

* prior art
Nothing similar exists (to my knowledge).

- proprietary platforms
  # I don't believe they can really work, at least without being partially open sources because of the vastness of data sources.

- perhaps the closest are Dogsheep, Perkeep

** Memex by Andrew Louis :noexport:
https://hyfen.net/memex
Very well built (Ruby?), but not open source :(
** Perkeep :noexport:
Seems to be centered on storage model (objects?).
E.g. I struggled to inspect objects, and it seems to be HTTP api-centric
# it does have some nice interfaces though
# Hope to give it a one more go, HPI can be used as the source of input data.
* humble goal: 'data mirror'
https://beepb00p.xyz/sad-infra.html#data_mirror

Separation of concerns!

- something that runs on the client side and syncs the data locally
  'export layer'
- data bindings to interpret the data stored locally
  'data access layer' (DAL)

Ideally this would be provided by the service instead

** fun fact
Phone apps are often basically data mirrors.

- runs on client, syncs data to cache (usually sqlite database)
  # at least on android
- relatively easy to interact with the database
  # ... if not for hostile ecosystem which prevents me from seeing my own data

* ... how it's going?
https://beepb00p.xyz/myinfra.html

https://beepb00p.xyz/myinfra_files/myinfra.svg

* architecture: export layer
Export data from the outside worlds to the filesystem.

https://beepb00p.xyz/exports.html#export_layer

Key principles:

- use existing API bindings
- keep raw data intact
- optimize for robust atomic exports
  # can alwasy clean up data later if necessary (during parsing)

** quasi-realtime
For now, periodic exports -- much simpler to implement.
# possible to make it 'almost' continuous by using streaming APIs or polling some endpoint
# just a bit harder so I didn't invest time in it so far
** TODO synthetic exports

* architecture: data access layer
Parse from raw data and reconstruct it.

** tyranny of databases
- choosing schema is hard
  # often you don't even know the schema, you have to reverse engineer it
  # if you're only picking certain attributes, you might miss on data if the API changes
  # if you're a hoarder like me it's inacceptable :)
- migrations are hard
- not everything fits into the relational model
  https://beepb00p.xyz/unnecessary-db.html
- sqlite types suck
  # not to spawn a typed vs untyped debate, but you
- databases do not forgive errors
  # bad database migration: you might ruin the data
  # bad 'normalising' -- your program crashes
  #   and even that is possible to work around defensively

** files are great!
- easy to understand and reason about
  # low entry barrier
  # e.g. in comparison do you remember how to make sure your database readers don't crash if anyone is writing into it
  # of course assumes 'immutable', append-only model?
- easy to interoperate (cmdline tools)
- easy to backup
- easy to sync (syncthing/dropbox)

** but databases are good!
Of course databases are very useful
- efficient storage
- fast access
- query language

** best of both worlds?
https://github.com/karlicoss/cachew#incremental-data-exports

Currently sqlite... one day postgres/redis?

* HPI: features
- local-first (actually fully offline!)
- enriching data
  # e.g. adding timezone information to the sleep data based on google location
** merging data
- github data is arbitrated between
  - GDPR export (manual, but complete)
  - API data (automatic, but incomplete)
    # because of API limit for 300 last events
- my sleep data is merged from
  - jawbone (now dead)
  - emfit ()
  - garmin (todo)
    # to improve data quality
- my exercise data is merged from
  - endomondo API exports (dead now)
  - open source RunnerUp app
    # outputs files! perfect api

* what's next?
# 7-10 minutes?
Rough ideas: https://beepb00p.xyz/exobrain/projects/hpi.html
** more user interfaces
# haven't done as many integrations as I wish I did
# TODO links?
- grafana
  # already started, but would be nice to make it a bit more automatic
- datasette
  https://datasette.io
  # existing rich ecosystem of tools for data exploration & publishing
  # 'downside' -- against sqlite databases
  # but possible to interface, cachew!
- solid
  https://solidproject.org/about
  # decentralized pods, apps
- memri
  # 'data browser'
  https://github.com/memri/pod
- openhumans.org
  https://www.openhumans.org/about
  # quantified self / data analysis notebooks
- perkeep
  https://perkeep.org
  # a set of open source formats, protocols
  #  and software for modeling, storing, searching, sharing and synchronizing data

** inter-language interfaces
For people who don't like Python ;)
# translate data to other languages
# but also for performance, interoperation, etc
# julia?
- apache arrow?
  https://arrow.apache.org
- json api =hpi query=
- HTTP api https://github.com/seanbreckenridge/HPI_API
** community
For everyone interested in data liberation & bulding memexes!
https://memex.zulipchat.com

** misc
- make setup & demonstrations easier
  # docker isn't that simple because images are basically immutable, and it's hard to tinker with them
  # TODO python packaging etc?
  # virtual environments etc
- more modules/data?
- realtime data sources

* questions?
- https://github.com/karlicoss/HPI#readme
- beepb00p.xyz
- twitter.com/karlicoss
- @karlicoss:matrix.org

* appendix: why Python?
- dataclasses
- mypy
- decorators
- iterators
- namespace packages
- malleable

* appendix: what's hard/unsolved?
- data is crap
  # It takes a while to reverse engineer it. E.g. even timezones etc
- data on phones is locked in
  Even more annoying that they often keep data in sqlite databases on the device already.
- how to scale/extend?
  # I can't properly maintain
  Sort of an Emacs problem
- versioning?
  kind of the same problem
  # completely unclear -- changes all the time
  # at least with code it's possible to keep it backwards compatible & test
- where to get test data?
  # would be nice if services provided test data/test API endpoints
  # or obfuscator/anonymizer for test data

* TODO reveal stuff :noexport:
** [2021-04-19 Mon 22:39] time estimates?
* What does it solve? :noexport:
** local/offline interfaces
Even if you do have internet, search on most sites sucks hard.
Even remembering where exactly you need to search is a cognitive overhead.
- TODO show F2 keybinding?
- orger
*** TODO link to search article?

** quantified self
We have lots of data, yet no insight from it.
# perhaps except the 'insight' big tech gets for ads purposes
Imagine if you could have a system which automatically finds interesting correlations and TODO

** siphons
** dead services
- =my.endomondo=
** migrating/lock-in protection
- =my.rtm=
  I'm not using Remember The Milk anymore, but have a data mirror, so I can search in old tasks.

** memex
* shared patterns (what is 'hpi' as a library?) :noexport:
Shared design principles for exporting data
This way it differs from a bunch of separate
Extracting in =my.core=
